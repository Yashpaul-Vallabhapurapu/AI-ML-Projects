# -*- coding: utf-8 -*-
"""Explainerdashboard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LPg0kWrpUX9MBrF1VRPIraeF5M8mJPww
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats 
import math
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

from google.colab import drive
drive.mount('/content/drive/')

path = '/content/drive/MyDrive/AI_Project/Final'

data = pd.read_csv(path+'/Coffee_Stores_Data.csv')
stores_data = data[data['StoreID'].isin([18,117,332])]
products = ['Honey Raisin Bran Muffin','Jalapeno Cheese Bagel', 'Lemon Loaf', 'Mixed Berries & Granola Yogurt Parfait', 'Mixed Fruit Snack Pot', 'Muffin - Blueberry Streusel', 'Muffin - Double Chocolate', 'New York Cheesecake', 'Plain Bagel', 'Pressed Juicery Spicy Greens w Ginger', 'Protein Box', 'Smoked Salmon Sandwich', 'Tasty Tuna Salad Sandwich', 'Vive Juice Shot']
dataset = stores_data[stores_data['Description'].isin(products)]

from pandas.tseries.holiday import USFederalHolidayCalendar as calendar
cal = calendar()
holidays = cal.holidays(start=dataset['BusinessDate'].min(), end=dataset['BusinessDate'].max())
#df.loc[df['A'] > 2, 'B'] = new_val
dataset['BusinessDate'] = pd.to_datetime(dataset['BusinessDate'])
dataset['Holiday'] = dataset['BusinessDate'].isin(holidays)
dataset["IsWeekend"] = dataset['BusinessDate'].dt.dayofweek > 4
dataset['Weekday'] =  dataset['BusinessDate'].dt.day_name()
#dataset['BusinessDate'] = dataset['BusinessDate'].astype(str)
#dataset[['Year','Month','Date']] = dataset.BusinessDate.str.split("-",expand=True)
dataset = dataset.loc[dataset['StoreID'].isin([18,117,332])]
drive_thru = {18:False,117:False,332:True}
dataset['DriveThru'] = dataset['StoreID'].map(drive_thru)
pasadena_weather = pd.read_excel(path+'/91101.xlsx')
sandeigo_weather = pd.read_excel(path+'/92122.xlsx')
pasadena_weather['BusinessDate'] = pd.to_datetime(pasadena_weather.BusinessDate)
sandeigo_weather['BusinessDate'] = pd.to_datetime(sandeigo_weather.BusinessDate)
d1=pd.merge((dataset[dataset['StoreID']==18]),pasadena_weather, on="BusinessDate", how="left").reset_index(drop=True)
d2=pd.merge((dataset[dataset['StoreID']==117]),pasadena_weather, on="BusinessDate", how="left").reset_index(drop=True)
d3=pd.merge((dataset[dataset['StoreID']==332]),sandeigo_weather, on="BusinessDate", how="left").reset_index(drop=True)
main_data = pd.concat([d1,d2,d3],ignore_index=True)

#main_data['BusinessDate'] = pd.to_datetime(main_data['BusinessDate']).astype(np.int64)
main_data = main_data.drop(['CategoryLvl1Desc','Description'],axis=1)
bool_list = ['Holiday','IsWeekend','DriveThru']
week_list = ['Weekday']
item_type = ['ItemType']
case_type= ['CategoryLvl2Desc']
categ_list = ['CategoryLvl3Desc']
def binary_map(x):
    return x.map({True: 1, False: 0})
def categ_map(q):
    return q.map({'Muffins & Scones':0,'Ready to Drink':1,'Ready to Eat':2,'Bagels':3,'Cakes & Breads':4})
def case_type_map(p):
    return p.map({'Bake Case': 1, 'Cold Case': 0})
def item_type_map(y):
    return y.map({'Core': 1, 'Unassigned': 0})
def weekday(z):
    return z.map({'Sunday':0, 'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6})
main_data[bool_list] = main_data[bool_list].apply(binary_map)
main_data[week_list] = main_data[week_list].apply(weekday)
main_data[item_type] = main_data[item_type].apply(item_type_map)
main_data[case_type] = main_data[case_type].apply(case_type_map)
main_data[categ_list] = main_data[categ_list].apply(categ_map)

main_data

main_data.columns

columns_list = ['SoldQuantity','PLU','ReceivedQuantity','EndQuantity','Holiday', 'IsWeekend', 'Weekday','Temp']
modified_data = main_data[columns_list]
df_input = modified_data
df_input

# Split dataset into features and target
y = modified_data.pop('SoldQuantity')
X = modified_data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)

from sklearn.datasets import make_regression
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
randomForest = RandomForestRegressor(max_depth=2, random_state=0)
gradientBoosting = GradientBoostingRegressor(learning_rate=0.2,n_estimators=100,max_depth=5)
randomForest.fit(X_train, y_train)
gradientBoosting.fit(X_train,y_train)

from lightgbm import LGBMRegressor
lgbm = LGBMRegressor(learning_rate=0.01,n_estimators=1440,max_depth=20)
lgbm.fit(X_train,y_train)

from xgboost import XGBRegressor,XGBRFRegressor
xgbr = XGBRegressor(learning_rate=0.1,n_estimators=1500,max_depth=2)
xgbr.fit(X_train,y_train)

from explainerdashboard import ClassifierExplainer, RegressionExplainer,ExplainerDashboard,ExplainerHub
randomForest_explainer = RegressionExplainer(randomForest, X_test, y_test)
gradientBoosting_explainer = RegressionExplainer(gradientBoosting, X_test, y_test)
lgbm_explainer  = RegressionExplainer(lgbm,X_test,y_test)
xgbr_explainer = RegressionExplainer(xgbr,X_test,y_test)

e1 = ExplainerDashboard(randomForest_explainer)
e2 = ExplainerDashboard(gradientBoosting_explainer)
e3 = ExplainerDashboard(lgbm_explainer)
e4 = ExplainerDashboard(xgbr_explainer)
#e4.run()

ExplainerHub([e1, e2]).run()

